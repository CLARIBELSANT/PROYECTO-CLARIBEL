---
title: "PROYECTO"
author: "Claribel Santiago"
date: "2023-05-25"
output: html_document
---



```{r, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(VIM)
library(mice)#imputar
library(dplyr)
library(ggcorrplot)#matrices de correlaciones y distancias
library(corrplot)#matrices de correlaciones y distancias}
library(aplpack)#CARITAS DE CHERNOFF
library(lattice)#GRÁFICO DE PARALELAS
library(RColorBrewer)
library(purrr)
library(kableExtra)
```
## BASE DE DATOS

* Cargamos las bases de datos *heroesInformation* y *characterStats*. Quitamos los nombres duplicados y unimos  *DC_Marvel* con *infoStats* según sus nombres.


```{r base-de-datos, results="hide"}
infoHeroes <- read.csv("~/PROYECTO-CLARIBEL/Data/heroesInformation.csv", na.strings = c("-", "-99", "-99.0")) 
infoStats <- read.csv("~/PROYECTO-CLARIBEL/Data/charactersStats.csv", na.strings = "")
colnames(infoHeroes)[2] <- "Name"

DC_Marvel <- infoHeroes %>% 
  filter(infoHeroes$Publisher %in% c("DC Comics", "Marvel Comics"))

duplicated(DC_Marvel$Name)
dim(DC_Marvel)
DC_Marvel <- DC_Marvel %>%
  filter(!duplicated(Name))

Dc_Marvel_StatsInfo <- left_join(DC_Marvel, infoStats, by = "Name")

```

* Usamos la función aggr() para calcular resúmenes estadísticos para subconjuntos de datos. En particular, queremos calcular los NA en nuestra base de datos. 
```{r, message=FALSE,warning=FALSE,results='hide'}
aggr_plot <- aggr(Dc_Marvel_StatsInfo, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(Dc_Marvel_StatsInfo), 
                 cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
```

```{r, message=FALSE,warning=FALSE,results='hide'}
Dc_Marvel_StatsInfo<-Dc_Marvel_StatsInfo[,-19]#SE MODIFICARÁ AL IMPUTAR
Dc_Marvel_StatsInfo <- Dc_Marvel_StatsInfo[,-12]#ESTÁ REPETIDO
Dc_Marvel_StatsInfo <- Dc_Marvel_StatsInfo[,-9]#ES LA COLUMNA CON MÁS NA
Dc_Marvel_StatsInfo<-Dc_Marvel_StatsInfo[,-1]#NO SABEMOS QUE SIGNIFICA X


```

* Para no reducir tanto la base de datos, cambiamos los valores con NA de las columnas no numéricas por "Undefined". 
```{r, message=FALSE,warning=FALSE,results='hide'}
Dc_Marvel_StatsInfo$Race[is.na(Dc_Marvel_StatsInfo$Race)] <- "Undefined"
Dc_Marvel_StatsInfo$Eye.color[is.na(Dc_Marvel_StatsInfo$Eye.color)] <- "Undefined"
Dc_Marvel_StatsInfo$Alignment.x[is.na(Dc_Marvel_StatsInfo$Alignment.x)] <- "Undefined"
Dc_Marvel_StatsInfo$Hair.color[is.na(Dc_Marvel_StatsInfo$Hair.color)] <- "Undefined"
Dc_Marvel_StatsInfo$Gender[is.na(Dc_Marvel_StatsInfo$Gender)] <- "Undefined"
```
* Los datos faltantes son
  * *Completatemente aleatorios* si la razón de la falta de datos es ajena a los datos mismos es decir, no depende de la categoría  del resto de las variables.
  * *No aleatorios* si no son completamente aleatorios.
  * *Aleatorio* si no depende de las categorías pero si puede depender de otras variables.

Como todos los datos faltantes que nos quedan son Aleatorios o completamente aleatorios usamos el método de imputación de datos.
Idealmente, queremos no cambiar la distribución de nuestros datos.\

*Imputación con la media*:
N=Antes de la imputación,
R=con la imputación.
```{r,warning=FALSE}
# https://rpubs.com/ydmarinb/429757
columns <- c("Weight", "Height")
imputed_data <- mice(Dc_Marvel_StatsInfo[,names(Dc_Marvel_StatsInfo) %in% columns],m = 1,
  maxit = 1, method = "mean",seed = 2018,print=F)
complete.data <- mice::complete(imputed_data)
par(mfrow=c(1,2))
plot(density(Dc_Marvel_StatsInfo$Weight,na.rm = T),col=1,main="Weight")
lines(density(complete.data$Weight),col=2)
plot(density(Dc_Marvel_StatsInfo$Height,na.rm = T),col=1,main="Height")
lines(density(complete.data$Height),col=2)


impute_arg<-round(as.data.frame(complete.data))

Dc_Marvel_StatsInfo["Weight"]<-impute_arg["Weight"]
Dc_Marvel_StatsInfo["Height"]<-impute_arg["Height"]

```


*Imputación mediante regresión*:
N=Antes de la imputación,
R=con la imputación.
```{r,warning=FALSE}
columns<-c("Intelligence","Strength")
impute_arg1  <- mice(Dc_Marvel_StatsInfo[,names(Dc_Marvel_StatsInfo) %in% columns],m = 1,
  maxit = 1, method = "norm.predict",seed = 2018,print=F)

impute_arg <- mice::complete(impute_arg1)
par(mfrow=c(1,2))
plot(density(Dc_Marvel_StatsInfo$Intelligence,na.rm =T),col=1,main="Intelligence")
lines(density(impute_arg$Intelligence),col=2)
plot(density(Dc_Marvel_StatsInfo$Strength,na.rm = T),col=1,main="Strength")
lines(density(impute_arg$Strength),col=2)

impute_arg<-round(as.data.frame(impute_arg))

Dc_Marvel_StatsInfo["Intelligence"]<-impute_arg["Intelligence"]
Dc_Marvel_StatsInfo["Strength"]<-impute_arg["Strength"]


```

*Imputación mediante regresión*:
N=Antes de la imputación,
R=con la imputación.
```{r,warning=FALSE}
columns<-c("Durability","Power")
impute_arg1  <- mice(Dc_Marvel_StatsInfo[,names(Dc_Marvel_StatsInfo) %in% columns],m = 1,
  maxit = 1, method = "norm.predict",seed = 2018,print=F)

impute_arg <- mice::complete(impute_arg1)
par(mfrow=c(1,2))
plot(density(Dc_Marvel_StatsInfo$Durability,na.rm =T),col=1,main="Durability")
lines(density(impute_arg$Durability),col=2)
plot(density(Dc_Marvel_StatsInfo$Power,na.rm = T),col=1,main="Power")
lines(density(impute_arg$Power),col=2)

impute_arg<-round(as.data.frame(impute_arg))

Dc_Marvel_StatsInfo["Durability"]<-impute_arg["Durability"]
Dc_Marvel_StatsInfo["Power"]<-impute_arg["Power"]

```




*Imputación mediante regresión*:
N=Antes de la imputación,
R=con la imputación.
```{r,warning=FALSE}
columns<-c("Speed","Combat")
impute_arg1  <- mice(Dc_Marvel_StatsInfo[,names(Dc_Marvel_StatsInfo) %in% columns],m = 1,
  maxit = 1, method = "norm.predict",seed = 2018,print=F)

impute_arg <- mice::complete(impute_arg1)
par(mfrow=c(1,2))
plot(density(Dc_Marvel_StatsInfo$Speed,na.rm =T),col=1,main="Speed")
lines(density(impute_arg$Speed),col=2)
plot(density(Dc_Marvel_StatsInfo$Combat,na.rm = T),col=1,main="Combat")
lines(density(impute_arg$Combat),col=2)

impute_arg<-round(as.data.frame(impute_arg))

Dc_Marvel_StatsInfo["Speed"]<-impute_arg["Speed"]
Dc_Marvel_StatsInfo["Combat"]<-impute_arg["Combat"]
```
* Después de haber imputado y eliminado algunas columnas. Verificamos que nuestra nueva base de datos no tiene columnas con NA. 
```{r, warning=FALSE, message=FALSE}

colSums(is.na(Dc_Marvel_StatsInfo))
```




## MUESTRA DE LA BASE DE DATOS
Dividimos nuestra nueva base de datos por compañía. En lo que sigue trabajaremos solamente con los heroes de DC_Marvel.\

* contamos con 15 variables y por lo tanto 15 varianzas y 105 covarianzas

```{r}
BASE_DC <- as.data.frame(subset(Dc_Marvel_StatsInfo, Publisher == "DC Comics"))  
summary(BASE_DC)
rownames (BASE_DC) <- BASE_DC$Name
BASE_DC

```

* En este caso las variables numéricas tienen escalas muy diferentes de medición (obtenemos valores negativos y mayores a 1000). Por esta razón trabajamos con el coeficiente de correlación.
* Hacemos un plot de la correlación, corrplot(), esto es importante para identificar la estructura y el patrón ocultos en la matriz.


```{r}
numDC<-dplyr::select_if(BASE_DC, is.numeric)
cov(numDC)
M<-cor(numDC)
M
corrplot(M, method = 'number')

```


### PLots gráficos multivariados
* Hacemos un scatterplot de las variables númericas y las coloreamos por los niveles de su género.\
  * En la altura con respecto al peso observemos que la mayoría de los personajes tienen un peso entre 0 y 200, altura entre 100 y 200. Los heroes hombres son los más altos, el peso máximo es el de un hombre mayor a 800. La mayoría de mujeres tienen un peso menor que 100 y altura entre 150 y 200.
Las mujeres tienen menor altura y menor peso en general. En general, la altura es proporcional al peso. 
  * Las otras variables no dependen de la altura ni del peso.
  * En el poder con respecto al combate observemos que el poder no depende del combate.
  * Los heroes hombres son los que tienen mayor poder y mayor combate.
  *Usamos los gráficos de paralelas, donde podemos observar la distribución de las variables numéricas en función de algún atributo (en este caso el género).
```{r, warning=FALSE,message=FALSE,fig.width=12, fig.height=12}
#install.packages("psych)
library(psych)
c<-c(6,9,10,11,12,13,14,15)
cl <- c("#FF33CC", "#CC66FF", "#33C9CC", "black")
pairs(BASE_DC[,c], pch=17, cex=1, col=cl[as.factor(BASE_DC$Gender)],
     main="Scatterplot DC Heroes", lower.panel = NULL, oma=c(3,3,3,15))
par(xpd = TRUE)
legend("bottomright", legend = c(levels(as.factor(BASE_DC$Gender))), pch=17, col=cl[1:3], pt.cex=1.2)
```


* Hacemos un plot de las variables altura y peso  y las coloreamos por los niveles de su alineación.
   * Hay muy pocos personajes con alineación neutral y solo un personaje con alineación indefinida. 
   * El personaje con más peso y altura es un villano. El personaje con menor altura y peso es un heroe.
   * La mayoría de heroes y villanos tienen peso menor a 200 y altura mayor a 150 y menor a 200. 


```{r, warning=FALSE,message=FALSE,fig.width=12, fig.height=12}
plot(BASE_DC$Height, BASE_DC$Weight, pch=1, cex=1.5, col=cl[as.factor(BASE_DC$Alignment.x)],
     main="Height and weight", xlab="Altura", ylab="Peso") 
legend("bottomright", legend = c(levels(as.factor(BASE_DC$Alignment.x))), pch=1, col=cl[1:4], pt.cex=1.5)
```

* Creamos una gráfica de barras relativa y absoluta de la cantidad de superhéroes por genéro.

```{r}
t<-table(BASE_DC$Gender)
barplot(t,ylim=c(0,200))

tr<-(table(BASE_DC$Gender)/206)*100
barplot(tr,ylim=c(0,80))
```

Obtener gráficas de paralelas, donde podemos observar la distribución de las variables numéricas en función de algún atributo (en este caso el género de los personajes)
```{r}
library(lattice)
c<-c(6,9,10,11,12,13,14,15)
parallelplot(~BASE_DC[,c] | Gender, data=BASE_DC)
```



### Análisis por clusters
* Ahora queremos encontrar patrones o grupos (clusters) dentro del conjunto.


```{r, warning=FALSE,message=FALSE,fig.width=12, fig.height=12}
# Recordemos los datos obtenidos al analizar varianzas y correlaciones
summary(numDC)
dim(numDC)
var(data.matrix(numDC))
cor.mat_all <- cor(data.matrix(numDC), use="complete.obs")
cor.mat_all
```

```{r,message=FALSE,warning=FALSE,results='hide'}
#  Como no queremos que el algoritmo de agrupamiento dependa de una unidad variable arbitraria, comenzamos escalando/estandarizando los datos usando la función R scale:
US_df <- scale(numDC)
head(US_df)
var(US_df)

#  Calculamos los valores de disimilitud con dist y luego usamos hclusty y especificamos el método de aglomeración que se usará (es decir, "completo", "promedio", etc.). Con la tabla comparativa de metodos y el coheficiente de correlación, vemos que el método optimo es average. Entonces podemos trazar el dendograma.

dist <- dist(US_df, method = "euclidean")
```


* Realizamos los clusters
```{r,message=FALSE,warning=FALSE}
cluster_single <- hclust (d = dist, method = 'single')
cluster_complete <- hclust (d = dist, method = 'complete')
cluster_average <- hclust (d = dist, method = 'average')
cluster_ward <- hclust (d = dist, method = 'ward.D2')


par (mfrow = c(2,2))
plot(cluster_single,cex=0.7, hang = -2)
plot(cluster_complete,cex=0.7, hang = -2)
plot(cluster_average,cex=0.7, hang = -2)
plot(cluster_ward,cex=0.7, hang = -2)

par (mfrow = c(1,1))
```



* Comparamos los coheficientes de correlación de los distintos métodos, y obtenemos una tabla comparativa donde se muestra que el método más optimos es "average" 

```{r, message=FALSE,warning=FALSE}
library(NbClust)
library(factoextra)

# Comparar coeficiente de correlación de los métodos

# vector con nombre de los métodos
m <- c( "average", "single", "complete", "ward.D2", "median", "centroid")
names(m) <- c( "average", "single", "complete", "ward.D2", "median", "centroid")

# Función para calcular el coeficiente de correlación
coef_cor <- function(x) {
  cor(x=dist, cophenetic(hclust(d=dist, method = x)))
}

# Tabla comparativa
coef_tabla <- map_dbl(m, coef_cor) 
coef_tabla
```

* Ahora vamos a calcular el número óptimo de clusters y realizar el cluster con el método y número óptimos.

```{r,warning=FALSE,message=FALSE}
# Aplicar todos los índices y métodos
res.nbclust <- NbClust(US_df, distance = "euclidean",
                       min.nc = 3, max.nc = 8, 
                       method = "average", index ="all")
```

```{r,fig.width=12, fig.height=12,warning=FALSE,message=FALSE}
# Realizar plot con el número óptimo de cluster y marcar los grupos
plot(cluster_average, cex = 0.6, hang = -2)
rect.hclust(cluster_average, k = 7, border = 2:8)
```


```{r,message=FALSE,warning=FALSE,fig.align=12,fig.height=15}


# Clusters
set.seed(12345)

hc_average <- hclust(d=dist, method = "average")
hc_complete <- hclust(d=dist, method = "complete")
hc_single <- hclust(d=dist, method = "single")
hc_centroid <- hclust(d=dist, method = "centroid")



fviz_dend(x = hc_average, k=7,
          cex = 0.3,
          main = "Cluster método average",
          xlab = "Personajes",
          ylab = "Distancias",
          type= "rectangle",
          sub = "",
          horiz = TRUE)+
  geom_hline(yintercept = 3.8, linetype = "dashed")


sub_grp <- cutree(hc_average, k = 7)
table(sub_grp)


fviz_cluster(list(data = US_df, cluster = sub_grp))


# Comparar dendrogramas
library(dendextend)

hc_single <- as.dendrogram(hc_single) 
hc_average <- as.dendrogram(hc_average)

tanglegram(rank_branches(hc_single),rank_branches(hc_average),
           main_left = "Single",
           main_right = "Average", lab.cex= 0.3, margin_inner = 5, k_branches = 7)
```



### Análisis de PCA

* El análisis de componenetes principales tiene como objetivo reducir la dimensión y conservar en lo posible su estructura, es decir la forma de la nube de datos para esto el primer paro es obtener la matriz de correlaciones y la matriz de covarianzas.
   * Como vimos antes es apropiado tabrajar con la matriz de correlaciones y vimos que hay muchas correlaciones altas, entonces se podría disminuir la dimensión y es apropiado hacer un análisis de componentesprincipales.
   * Para verificar que en efecto, la matriz es factorizable, aplicamos el criterio de Kaiser-Meyer-Olkin. La prueba de Kaiser-Meyer-Olkin (KMO) es una medida de qué tan adecuados son sus datos para el análisis factorial . La prueba mide la adecuación del muestreo para cada variable en el modelo y para el modelo completo. Dado que las variables son mayores a 80 en está escala podemos hacer la reducción.
   
   
```{r}
library(psych)
kmo.1 <-KMO(numDC)
kmo.1

```

* Calculamos los componentes principales con la función princomp.
   * Con la instrucción loadings lo que obtenemos son las cargas de los componentes principales. Es decir, lo que obtenemos son los coeficientes que se usarán en las combinaciones lineales para calcular cada componente principal. Por ejemplo, para un conjunto de datos x1,x2,...,xp
la primera columna que está denotada por Comp.1 tiene los coeficientes  
a11,a21,...,ap1 de la combinación lineal con la que se obtiene la primera componente principal:Z1=a11x1+a21x2+...ap1xp.

```{r,warning=FALSE,message=FALSE}
cp1 <- princomp(numDC, cor = TRUE)
summary(cp1)
```

 * Esta primera componente principal será la que refleje la mayor variabilidad de los datos. Las cargas son las entradas de cada eigenvector son las siguientes:
 
```{r,warning=FALSE,message=FALSE}
cp1$loadings 
```

 La componente principal se escribiría como $$
(0.112,0.388,0.409,0.389,0.437,417,0.388)(100m-mean(100m),200m−mean(200m),...,Marathon−mean(Marathon))$$

 * Las desviaciones estándar son las raíces de las lambdas.

```{r, message=FALSE,warning=FALSE}
cp1$sdev
```

* Para calcular la varianza debemos elevar al cuadrado cada una de las entradas.

```{r, message=FALSE,warning=FALSE}
varianza.cp1 <- (cp1$sdev)^2
varianza.cp1
```

* Una forma de ver con cuantos componentes nos conviene trabajar es con el gráfico de codo.

```{r}
screeplot(cp1, type="lines")
```

* Usaremos la función prcomp() y primero realizaremos el análisis sin quitar la variable anterior.

```{r}
res.pca <- prcomp(numDC, scale = TRUE )
```

* Vamos a graficar a los personajes y corolearlos por perfiles similares.

```{r}
fviz_pca_ind(res.pca,
             col.ind = "cos2", # color por la calidad de representación
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # evitar que se sobrelape el texto
             )


fviz_pca_var(res.pca,
             col.var = "contrib", # Color por contribución al PCA
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE    
             )

```


### Interpretar resultados
 * Los resultados los fuimos interpretando en cada paso de análisis de este proyecto

### Resúmen y conclusiones
 * AL quitar los NA usamos los métodos de eliminación (para la columna con más NA), sustitución (para los valores no númericos) y imputación. En este último fuimos escogiendo la imputación adecuada (que elegimos basándonos en el plot de densidad). El nétodo usado al hacer la imputación de los datos fue el más óptimo y al observat las gráficas de densidad concluimos que fue muy bueno. Filtrando los personajes de DC, obtuvimos una base de datos a la que llamamos BASE_DC la cuál no contenía valores faltantes ni valores con NA y contaba solo con los datos de DC. Tambíen definimos una base que seleccionaba de BASE_DC, solo los datos numéricos, a esta la llamamos numDC.
 
 * Realizamos las matrices de correlación y covarianza, y concluimos que era más apropiado escalar y trabajar con la matriz de correlación. Las variables con mayor y menor correlación son combate-inteligencia y velocidad-altura respectivamente.
 

* Realizamos algunos plots multivariados:
  * Un scaterplot de los datos de numDC en donde se observaba la relación entre 2 a 2 de las  varriables y lo pintamos por género de los personajes. Obtuvimos qué hay algunos variables que son independientes de otras (ej. Ninguna de las variable dependen de la altura ni del peso) y otras que si (ej. altura y peso).
  
  * Hicimos un plot de las variables altura y peso y las coloreamos por los niveles de su alineación. Concluimos que:\
hay muy pocos personajes con alineación neutral y solo un personaje con alineación indefinida.\
El personaje con más peso y altura es un villano.\ El personaje con menor altura y peso es un heroe.\
La mayoría de heroes y villanos tienen peso menor a 200 y altura mayor a 150 y menor a 200.

  * Creamos una gráfica de barras relativa y absoluta de la cantidad de superhéroes por género. Concluimmos que:\ 
Hay poco más de 50 mujeres y aproximademente 150 hombres,es decir, hay mucho más hombres que mujeres. Esto último se observa mejor en la grafica de barras con porcentajes.


* En el anális por cluster, concluimos que es mejor ocupar el método de average y el número óptimo de  clusters es 7. Al final, hicimos el cluster plot mostrando esta agrupacion y comparamos averaje con single en un dendograma.

* En el análisis de componentes principales (PCA), aplicamos el criterio (KMO), la cuál nos indicaba qué tan adecuados son los datos para el análisis factorial. Dado que lamayoría de las variables (excepto dos) son mayores a 80 en está escala podemos hacer la reducción de dimensiones.
  * Calculamos los componentes principales con la función princomp. Obtuvimos datos como la desviación estandar, proporción de varianza y proporción acumulativa con 1, 2,..,7 y 8 componentes. 
  
  * Para saber con cuantos componentes nos conviene trabajar hicimos el gráfico de codo. Ahí concluimmos que es mejor trabajar con 2 o tal vez 3 componentes, pero dado a que es más facil interpretar 2 dimenciones, escogemos la primera opción. Finalmente, graficamos a los personajes y los coroleamos por perfiles similares.













