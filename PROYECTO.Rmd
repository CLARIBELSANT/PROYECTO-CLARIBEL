---
title: "PROYECTO"
author: "Claribel Santiago"
date: "2023-05-25"
output: html_document
---



```{r, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(VIM)
library(mice)#imputar
library(dplyr)
library(ggcorrplot)#matrices de correlaciones y distancias
library(corrplot)#matrices de correlaciones y distancias}
library(aplpack)#CARITAS DE CHERNOFF
library(lattice)#GRÁFICO DE PARALELAS
library(RColorBrewer)
library(purrr)
library(kableExtra)
```
## BASE DE DATOS

* Cargamos las bases de datos *heroesInformation* y *characterStats*. Quitamos los nombres duplicados y unimos  *DC_Marvel* con *infoStats* según sus nombres.


```{r base-de-datos, results="hide"}
infoHeroes <- read.csv("~/PROYECTO-CLARIBEL/Data/heroesInformation.csv", na.strings = c("-", "-99", "-99.0")) 
infoStats <- read.csv("~/PROYECTO-CLARIBEL/Data/charactersStats.csv", na.strings = "")
colnames(infoHeroes)[2] <- "Name"

DC_Marvel <- infoHeroes %>% 
  filter(infoHeroes$Publisher %in% c("DC Comics", "Marvel Comics"))

duplicated(DC_Marvel$Name)
dim(DC_Marvel)
DC_Marvel <- DC_Marvel %>%
  filter(!duplicated(Name))

Dc_Marvel_StatsInfo <- left_join(DC_Marvel, infoStats, by = "Name")

```

* Usamos la función aggr() para calcular resúmenes estadísticos para subconjuntos de datos. En particular, queremos calcular los NA en nuestra base de datos. 
```{r, message=FALSE,warning=FALSE,results='hide'}
aggr_plot <- aggr(Dc_Marvel_StatsInfo, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(Dc_Marvel_StatsInfo), 
                 cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
```

```{r, message=FALSE,warning=FALSE,results='hide'}
Dc_Marvel_StatsInfo<-Dc_Marvel_StatsInfo[,-19]#SE MODIFICARÁ AL IMPUTAR
Dc_Marvel_StatsInfo <- Dc_Marvel_StatsInfo[,-12]#ESTÁ REPETIDO
Dc_Marvel_StatsInfo <- Dc_Marvel_StatsInfo[,-9]#ES LA COLUMNA CON MÁS NA
Dc_Marvel_StatsInfo<-Dc_Marvel_StatsInfo[,-1]#NO SABEMOS QUE SIGNIFICA X


```

* Para no reducir tanto la base de datos, cambiamos los valores con NA de las columnas no numéricas por "Undefined". 
```{r, message=FALSE,warning=FALSE,results='hide'}
Dc_Marvel_StatsInfo$Race[is.na(Dc_Marvel_StatsInfo$Race)] <- "Undefined"
Dc_Marvel_StatsInfo$Eye.color[is.na(Dc_Marvel_StatsInfo$Eye.color)] <- "Undefined"
Dc_Marvel_StatsInfo$Alignment.x[is.na(Dc_Marvel_StatsInfo$Alignment.x)] <- "Undefined"
Dc_Marvel_StatsInfo$Hair.color[is.na(Dc_Marvel_StatsInfo$Hair.color)] <- "Undefined"
Dc_Marvel_StatsInfo$Gender[is.na(Dc_Marvel_StatsInfo$Gender)] <- "Undefined"
```
* Los datos faltantes son
  * *Completatemente aleatorios* si la razón de la falta de datos es ajena a los datos mismos es decir, no depende de la categoría  del resto de las variables.
  * *No aleatorios* si no son completamente aleatorios.
  * *Aleatorio* si no depende de las categorías pero si puede depender de otras variables.

Como todos los datos faltantes que nos quedan son Aleatorios o completamente aleatorios usamos el método de imputación de datos.
Idealmente, queremos no cambiar la distribución de nuestros datos.\

*Imputación con la media*:
N=Antes de la imputación,
R=con la imputación.
```{r,warning=FALSE}
# https://rpubs.com/ydmarinb/429757
columns <- c("Weight", "Height")
imputed_data <- mice(Dc_Marvel_StatsInfo[,names(Dc_Marvel_StatsInfo) %in% columns],m = 1,
  maxit = 1, method = "mean",seed = 2018,print=F)
complete.data <- mice::complete(imputed_data)
par(mfrow=c(1,2))
plot(density(Dc_Marvel_StatsInfo$Weight,na.rm = T),col=1,main="Weight")
lines(density(complete.data$Weight),col=2)
plot(density(Dc_Marvel_StatsInfo$Height,na.rm = T),col=1,main="Height")
lines(density(complete.data$Height),col=2)


impute_arg<-round(as.data.frame(complete.data))

Dc_Marvel_StatsInfo["Weight"]<-impute_arg["Weight"]
Dc_Marvel_StatsInfo["Height"]<-impute_arg["Height"]

```


*Imputación mediante regresión*:
N=Antes de la imputación,
R=con la imputación.
```{r,warning=FALSE}
columns<-c("Intelligence","Strength")
impute_arg1  <- mice(Dc_Marvel_StatsInfo[,names(Dc_Marvel_StatsInfo) %in% columns],m = 1,
  maxit = 1, method = "norm.predict",seed = 2018,print=F)

impute_arg <- mice::complete(impute_arg1)
par(mfrow=c(1,2))
plot(density(Dc_Marvel_StatsInfo$Intelligence,na.rm =T),col=1,main="Intelligence")
lines(density(impute_arg$Intelligence),col=2)
plot(density(Dc_Marvel_StatsInfo$Strength,na.rm = T),col=1,main="Strength")
lines(density(impute_arg$Strength),col=2)

impute_arg<-round(as.data.frame(impute_arg))

Dc_Marvel_StatsInfo["Intelligence"]<-impute_arg["Intelligence"]
Dc_Marvel_StatsInfo["Strength"]<-impute_arg["Strength"]


```

*Imputación mediante regresión*:
N=Antes de la imputación,
R=con la imputación.
```{r,warning=FALSE}
columns<-c("Durability","Power")
impute_arg1  <- mice(Dc_Marvel_StatsInfo[,names(Dc_Marvel_StatsInfo) %in% columns],m = 1,
  maxit = 1, method = "norm.predict",seed = 2018,print=F)

impute_arg <- mice::complete(impute_arg1)
par(mfrow=c(1,2))
plot(density(Dc_Marvel_StatsInfo$Durability,na.rm =T),col=1,main="Durability")
lines(density(impute_arg$Durability),col=2)
plot(density(Dc_Marvel_StatsInfo$Power,na.rm = T),col=1,main="Power")
lines(density(impute_arg$Power),col=2)

impute_arg<-round(as.data.frame(impute_arg))

Dc_Marvel_StatsInfo["Durability"]<-impute_arg["Durability"]
Dc_Marvel_StatsInfo["Power"]<-impute_arg["Power"]

```




*Imputación mediante regresión*:
N=Antes de la imputación,
R=con la imputación.
```{r,warning=FALSE}
columns<-c("Speed","Combat")
impute_arg1  <- mice(Dc_Marvel_StatsInfo[,names(Dc_Marvel_StatsInfo) %in% columns],m = 1,
  maxit = 1, method = "norm.predict",seed = 2018,print=F)

impute_arg <- mice::complete(impute_arg1)
par(mfrow=c(1,2))
plot(density(Dc_Marvel_StatsInfo$Speed,na.rm =T),col=1,main="Speed")
lines(density(impute_arg$Speed),col=2)
plot(density(Dc_Marvel_StatsInfo$Combat,na.rm = T),col=1,main="Combat")
lines(density(impute_arg$Combat),col=2)

impute_arg<-round(as.data.frame(impute_arg))

Dc_Marvel_StatsInfo["Speed"]<-impute_arg["Speed"]
Dc_Marvel_StatsInfo["Combat"]<-impute_arg["Combat"]
```
* Después de haber imputado y eliminado algunas columnas. Verificamos que nuestra nueva base de datos no tiene columnas con NA. 
```{r, warning=FALSE, message=FALSE}

colSums(is.na(Dc_Marvel_StatsInfo))
```




## MUESTRA DE LA BASE DE DATOS
Dividimos nuestra nueva base de datos por compañía. En lo que sigue trabajaremos solamente con los heroes de DC_Marvel.\

* contamos con 15 variables y por lo tanto 15 varianzas y 105 covarianzas

```{r}
BASE_DC <- as.data.frame(subset(Dc_Marvel_StatsInfo, Publisher == "DC Comics"))  
summary(BASE_DC)
```

* En este caso las variables numéricas tienen escalas muy diferentes de medición (obtenemos valores negativos y mayores a 1000). Por esta razón trabajamos con el coeficiente de correlación.
* Hacemos un plot de la correlación, corrplot(), esto es importante para identificar la estructura y el patrón ocultos en la matriz.


```{r}
numDC<-dplyr::select_if(BASE_DC, is.numeric)

cov(numDC)
M<-cor(numDC)
M
corrplot(M, method = 'number')

```


### PLots gráficos multivariados
* Hacemos un scatterplot de las variables númericas y las coloreamos por los niveles de su género.\
  * En la altura con respecto al peso observemos que la mayoría de los personajes tienen un peso entre 0 y 200, altura entre 100 y 200. Los heroes hombres son los más altos, el peso máximo es el de un hombre mayor a 800. La mayoría de mujeres tienen un peso menor que 100 y altura entre 150 y 200.
Las mujeres tienen menor altura y menor peso en general. En general, la altura es proporcional al peso. 
  * Las otras variables no dependen de la altura ni del peso.
  * En el poder con respecto al combate observemos que el poder no depende del combate.
  * Los heroes hombres son los que tienen mayor poder y mayor combate.
 
```{r, warning=FALSE,message=FALSE,fig.width=12, fig.height=12}
#install.packages("psych)
library(psych)
c<-c(6,9,10,11,12,13,14,15)
cl <- c("#FF33CC", "#CC66FF", "#33C9CC", "black")
pairs(BASE_DC[,c], pch=17, cex=1, col=cl[as.factor(BASE_DC$Gender)],
     main="Scatterplot DC Heroes", lower.panel = NULL, oma=c(3,3,3,15))
par(xpd = TRUE)
legend("bottomright", legend = c(levels(as.factor(BASE_DC$Gender))), pch=17, col=cl[1:3], pt.cex=1.2)
```


* Hacemos un plot de las variables altura y peso  y las coloreamos por los niveles de su alineación.
   * Hay muy pocos personajes con alineación neutral y solo un personaje con alineación indefinida. 
   * El personaje con más peso y altura es un villano. El personaje con menor altura y peso es un heroe.
   * La mayoría de heroes y villanos tienen peso menor a 200 y altura mayor a 150 y menor a 200. 


```{r, warning=FALSE,message=FALSE,fig.width=12, fig.height=12}
plot(BASE_DC$Height, BASE_DC$Weight, pch=1, cex=1.5, col=cl[as.factor(BASE_DC$Alignment.x)],
     main="Height and weight", xlab="Altura", ylab="Peso") 
legend("bottomright", legend = c(levels(as.factor(BASE_DC$Alignment.x))), pch=1, col=cl[1:4], pt.cex=1.5)
```

### Análisis por clusters
* Ahora queremos encontrar patrones o grupos (clusters) dentro del conjunto.


```{r, warning=FALSE,message=FALSE,fig.width=12, fig.height=12}
# Recordemos los datos obtenidos al analizar varianzas y correlaciones
summary(numDC)
dim(numDC)
var(data.matrix(numDC))
cor.mat_all <- cor(data.matrix(numDC), use="complete.obs")
cor.mat_all
```

```{r,message=FALSE,warning=FALSE,results='hide'}
#  Como no queremos que el algoritmo de agrupamiento dependa de una unidad variable arbitraria, comenzamos escalando/estandarizando los datos usando la función R scale:
US_df <- scale(numDC)
head(US_df)
var(US_df)

#  Calculamos los valores de disimilitud con dist y luego usamos hclusty y especificamos el método de aglomeración que se usará (es decir, "completo", "promedio", etc.). Con la tabla comparativa de metodos y el coheficiente de correlación, vemos que el método optimo es average. Entonces podemos trazar el dendograma.

dist <- dist(US_df, method = "euclidean")
```


* Realizamos los clusters
```{r,message=FALSE,warning=FALSE}
cluster_single <- hclust (d = dist, method = 'single')
cluster_complete <- hclust (d = dist, method = 'complete')
cluster_average <- hclust (d = dist, method = 'average')
cluster_ward <- hclust (d = dist, method = 'ward.D2')


par (mfrow = c(2,2))
plot(cluster_single,cex=0.7, hang = -2)
plot(cluster_complete,cex=0.7, hang = -2)
plot(cluster_average,cex=0.7, hang = -2)
plot(cluster_ward,cex=0.7, hang = -2)

par (mfrow = c(1,1))
```



* Comparamos los coheficientes de correlación de los distintos métodos, y obtenemos una tabla comparativa donde se muestra que el método más optimos es "average" 

```{r, message=FALSE,warning=FALSE}
library(NbClust)
library(factoextra)

# Comparar coeficiente de correlación de los métodos

# vector con nombre de los métodos
m <- c( "average", "single", "complete", "ward.D2", "median", "centroid")
names(m) <- c( "average", "single", "complete", "ward.D2", "median", "centroid")

# Función para calcular el coeficiente de correlación
coef_cor <- function(x) {
  cor(x=dist, cophenetic(hclust(d=dist, method = x)))
}

# Tabla comparativa
coef_tabla <- map_dbl(m, coef_cor) 
coef_tabla
```

* Ahora vamos a calcular el número óptimo de clusters y realizar el cluster con el método y número óptimos.

```{r,warning=FALSE,message=FALSE}
# Aplicar todos los índices y métodos
res.nbclust <- NbClust(US_df, distance = "euclidean",
                       min.nc = 3, max.nc = 8, 
                       method = "average", index ="all")
```

```{r,fig.width=12, fig.height=12,warning=FALSE,message=FALSE}
# Realizar plot con el número óptimo de cluster y marcar los grupos
plot(cluster_average, cex = 0.6, hang = -2)
rect.hclust(cluster_average, k = 7, border = 2:8)
```


```{r,message=FALSE,warning=FALSE,fig.align=12,fig.height=15}


# Clusters
set.seed(12345)

hc_average <- hclust(d=dist, method = "average")
hc_complete <- hclust(d=dist, method = "complete")
hc_single <- hclust(d=dist, method = "single")
hc_centroid <- hclust(d=dist, method = "centroid")



fviz_dend(x = hc_average, k=7,
          cex = 0.3,
          main = "Cluster método aver",
          xlab = "Ciudades",
          ylab = "Distancias",
          type= "rectangle",
          sub = "",
          horiz = TRUE)+
  geom_hline(yintercept = 3.8, linetype = "dashed")


sub_grp <- cutree(hc_average, k = 7)
table(sub_grp)


fviz_cluster(list(data = US_df, cluster = sub_grp))


# Comparar dendrogramas
library(dendextend)

hc_single <- as.dendrogram(hc_single) 
hc_average <- as.dendrogram(hc_average)

tanglegram(rank_branches(hc_single),rank_branches(hc_average),
           main_left = "Single",
           main_right = "Average", lab.cex= 0.7, margin_inner = 5, k_branches = 7)
```



### Análisis de PCA




### Interpretar resultados


### Conclusiones


###Conclusiones










